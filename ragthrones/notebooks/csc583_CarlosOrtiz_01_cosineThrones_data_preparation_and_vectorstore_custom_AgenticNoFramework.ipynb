{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**CSC 583 - Final Project - Carlos Ortiz**\n",
        "\n",
        " Data Collection, End-to-End Data Preparation & Vector Store Notebook, Custom Agentic"
      ],
      "metadata": {
        "id": "-FKvoLhugN8-"
      },
      "id": "-FKvoLhugN8-"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "3100f470",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "3100f470",
        "outputId": "9fc367db-44dc-4052-87e2-c15b2fa3fb34",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: srt in /usr/local/lib/python3.12/dist-packages (3.5.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: rank-bm25 in /usr/local/lib/python3.12/dist-packages (0.2.2)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.0)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (1.3.5)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.8.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (18.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.5)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (6.0.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.23.2)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.76.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.0.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.20.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (34.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.3)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.4)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.3.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.11.12)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.3.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.29.0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.43.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.38.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.38.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.59b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.59b0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.0.8)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.0)\n",
            "Requirement already satisfied: rank-bm25 in /usr/local/lib/python3.12/dist-packages (0.2.2)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.6 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.1.0)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (2.8.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.0)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.44)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.45)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (1.33)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.11.12)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.6->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy srt regex tqdm spacy rank-bm25 faiss-cpu chromadb openai python-dotenv kaggle pyarrow requests beautifulsoup4 lxml datasets\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install langchain langchain-openai langchain-community sentence-transformers faiss-cpu rank-bm25"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COSINE of Thrones ‚Äî End-to-End Data Preparation & Vector Store Notebook\n",
        "\n",
        "This notebook builds the complete retrieval foundation for the **COSINE of Thrones** project.  \n",
        "It transforms raw Game of Thrones subtitles and character-lore datasets into a high-quality, augmented\n",
        "vector store used by the downstream RAG, evaluation, and agentic LangGraph-style modules.\n",
        "\n",
        "The workflow performs four major phases:\n",
        "\n",
        "### **1. Secure Environment Setup**\n",
        "Environment variables (OpenAI key, Kaggle credentials) are loaded using `python-dotenv`.  \n",
        "API keys are never hardcoded, ensuring the workflow is secure, portable, and version-controlled safely.\n",
        "\n",
        "### **2. Data Acquisition (Kaggle + Hugging Face + Web Crawler)**\n",
        "- Downloads raw subtitle files (`season1.json` ‚Ä¶ `season7.json`) from Kaggle.  \n",
        "- Parses and normalizes every subtitle line into a structured chronological DataFrame.  \n",
        "- Downloads the **Tuana/game-of-thrones** character-lore dataset from Hugging Face.  \n",
        "- Optionally crawls FunTrivia to collect human-written GOT QA for later evaluation.\n",
        "\n",
        "### **3. Chunking & Dataset Augmentation**\n",
        "Subtitle lines are windowed into overlapping narrative chunks.  \n",
        "Lore entries are converted into parallel chunks with metadata (`chunk_kind = \"character_lore\"`).  \n",
        "The two sources are merged into **one unified dataset (`df_aug`)**.\n",
        "\n",
        "### **4. Vector Store Construction (Embeddings + FAISS + BM25)**\n",
        "The notebook computes or loads:\n",
        "- OpenAI text embeddings (`text-embedding-3-large`)  \n",
        "- A normalized FAISS index for dense semantic search  \n",
        "- A BM25 token-index for lexical matching  \n",
        "- A hybrid retrieval function that blends both signals\n",
        "\n",
        "All artifacts are saved into `data/` so Phase 1-D can reload them instantly without recomputation.\n",
        "\n",
        "---\n",
        "\n",
        "**Output of this notebook:**  \n",
        "A complete, ready-to-query retrieval stack used by the COSINE of Thrones RAG pipeline, evaluation suite, and custom agentic graph.  \n",
        "This file is the foundation that powers all narrative-consistency experiments and downstream LangGraph-style workflows.\n",
        "\n",
        "### **5. COSINE of Thrones ‚Äî Custom Agentic RAG System**\n",
        "(Hand-Built **LangGraph-Style Pipeline)\n",
        "\n",
        "In this notebook, we construct a **fully custom agentic RAG pipeline** inspired by LangGraph, but implemented entirely in plain Python ‚Äî **no LangChain, no LangGraph, no framework dependencies**.\n",
        "\n",
        "This system behaves like a miniature graph-orchestrated agent, with state flowing through a sequence of nodes that each perform a specific cognitive function. The design mirrors the way modern agentic LLM systems coordinate parsing, retrieval, reasoning, and synthesis, but is engineered from scratch to provide complete transparency and control.\n",
        "\n",
        "### **What This Agentic Pipeline Does**\n",
        "The agent operates through four explicit \"nodes\", each a pure Python function:\n",
        "\n",
        "1. **Query Parser**  \n",
        "   Uses spaCy to detect named entities and infer the question type (who/what/when/why).  \n",
        "   Annotates the shared state with structured metadata for downstream reasoning.\n",
        "\n",
        "2. **Retriever**  \n",
        "   Invokes your upgraded `hybrid_search_aug()` function:  \n",
        "   a blend of FAISS vector search + BM25 lexical scoring + optional entity boosting  \n",
        "   over the merged subtitles + lore dataset (`df_aug`).\n",
        "\n",
        "3. **Reranker (Optional)**  \n",
        "   If available, a cross-encoder (ms-marco-MiniLM) reorders retrieved chunks by semantic relevance.  \n",
        "   If not available, retrieval results flow through untouched.\n",
        "\n",
        "4. **Synthesizer (LLM Answer Agent)**  \n",
        "   Formats evidence into a tightly controlled, citation-first prompt.  \n",
        "   Calls the generation model (e.g., `gpt-4o-mini`) and produces a grounded, one-sentence answer  \n",
        "   that must rely **only** on visible evidence.\n",
        "\n",
        "### **The State Object (Your Own LangGraph Alternative)**\n",
        "A custom `RAGState` dataclass stores all intermediate values:\n",
        "\n",
        "- the user question  \n",
        "- extracted entities  \n",
        "- question type  \n",
        "- retrieved chunks  \n",
        "- reranked chunks  \n",
        "- formatted evidence  \n",
        "- final answer  \n",
        "- per-node debug logs  \n",
        "\n",
        "This is analogous to LangGraph‚Äôs ‚Äúshared state passing‚Äù but implemented using clean Python dataclasses.\n",
        "\n",
        "### **Manual Graph Runner**\n",
        "Your pipeline is executed via:\n",
        "\n",
        "```python\n",
        "state = run_graph(question)"
      ],
      "metadata": {
        "id": "iXibLV8kfIxZ"
      },
      "id": "iXibLV8kfIxZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Environment Setup with python-dotenv and Kaggle Credentials\n",
        "\n",
        "To keep API keys and dataset credentials secure, this notebook uses the python-dotenv package. The .env file stores sensitive values such as the OpenAI API key or Kaggle tokens so they never appear in source control.\n",
        "\n",
        "The following snippet loads the .env file at runtime and injects the necessary environment variables into the notebook session. This approach keeps your workflow clean, portable, and compliant with best security practices."
      ],
      "metadata": {
        "id": "ZDWj25moGA8j"
      },
      "id": "ZDWj25moGA8j"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "db693df1",
      "metadata": {
        "id": "db693df1"
      },
      "outputs": [],
      "source": [
        "# # Get Open AI key from .env\n",
        "# import os\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key_here\"\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading the Game of Thrones Subtitle Dataset from Kaggle\n",
        "\n",
        "This notebook pulls the Game of Thrones subtitle dataset directly from Kaggle.  \n",
        "To keep credentials secure, the workflow loads Kaggle variables from the `.env` file using `python-dotenv`.  \n",
        "The logic supports both a JSON-packed `KAGGLE_API_KEY` or the standard `KAGGLE_USERNAME` and `KAGGLE_KEY` format.\n",
        "\n",
        "After loading the credentials, the Kaggle API authenticates and downloads the subtitle files into the `./data` directory.  \n",
        "These SRT files become the raw source material for all downstream preprocessing, cleaning, chunking, embedding, and RAG evaluation."
      ],
      "metadata": {
        "id": "EBhqKdSwG8vm"
      },
      "id": "EBhqKdSwG8vm"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4e20bb68",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4e20bb68",
        "outputId": "cc123cdc-25db-49fb-c24c-63dc5c5ffec2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset 'gunnvant/game-of-thrones-srt' from Kaggle to ./data ...\n",
            "Dataset URL: https://www.kaggle.com/datasets/gunnvant/game-of-thrones-srt\n",
            "Dataset downloaded and unzipped successfully.\n"
          ]
        }
      ],
      "source": [
        "# Get from Kaggle - https://www.kaggle.com/datasets/gunnvant/game-of-thrones-srt\n",
        "from dotenv import load_dotenv\n",
        "import os, json\n",
        "load_dotenv()  # call early\n",
        "\n",
        "kaggle_api = os.getenv(\"KAGGLE_API_KEY\")\n",
        "if kaggle_api:\n",
        "    try:\n",
        "        creds = json.loads(kaggle_api)\n",
        "        os.environ[\"KAGGLE_USERNAME\"] = creds.get(\"username\", os.getenv(\"KAGGLE_USERNAME\"))\n",
        "        os.environ[\"KAGGLE_KEY\"] = creds.get(\"key\", os.getenv(\"KAGGLE_KEY\"))\n",
        "    except Exception:\n",
        "        # fall back to separate vars if JSON parse fails\n",
        "        pass\n",
        "\n",
        "# Ensure both vars exist\n",
        "if not (os.getenv(\"KAGGLE_USERNAME\") and os.getenv(\"KAGGLE_KEY\")):\n",
        "    raise EnvironmentError(\"Set KAGGLE_USERNAME and KAGGLE_KEY in .env\")\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "\n",
        "# Initialize and authenticate the Kaggle API\n",
        "api = KaggleApi()\n",
        "try:\n",
        "    api.authenticate()\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Failed to authenticate Kaggle API: {e}\")\n",
        "\n",
        "# Define the Kaggle dataset path\n",
        "dataset_path = \"gunnvant/game-of-thrones-srt\"  # Replace if needed\n",
        "\n",
        "# Download and unzip the dataset to the ./data directory\n",
        "os.makedirs(\"./data\", exist_ok=True)\n",
        "print(f\"Downloading dataset '{dataset_path}' from Kaggle to ./data ...\")\n",
        "api.dataset_download_files(dataset_path, path=\"./data\", unzip=True)\n",
        "print(\"Dataset downloaded and unzipped successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "40e82e4a",
      "metadata": {
        "id": "40e82e4a",
        "outputId": "0511aac4-c3f0-4ff3-9a1d-c41cf865bd88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Loading season1.json ...\n",
            "üì• Loading season2.json ...\n",
            "üì• Loading season3.json ...\n",
            "üì• Loading season4.json ...\n",
            "üì• Loading season5.json ...\n",
            "üì• Loading season6.json ...\n",
            "üì• Loading season7.json ...\n",
            "üé¨ Loaded 44890 subtitle lines\n",
            "üéûÔ∏è Built 14990 subtitle chunks\n",
            "\n",
            "üì• Loading Tuana/game-of-thrones (character lore)...\n",
            "üìö Built 2357 lore chunks\n",
            "\n",
            "üîó Merged dataset now has 17347 total chunks\n",
            "üíæ Saved enriched + chunked dataset ‚Üí data/got_augmented_lore.csv\n"
          ]
        }
      ],
      "source": [
        "# ==========================\n",
        "# Phase 1 ‚Äî Build Full Chunk Dataset (Subtitles + Lore)\n",
        "# ==========================\n",
        "import os, json, ast\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "\n",
        "RAW_JSON_DIR = Path(\"data\")  # directory containing season1.json ... season7.json\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. Load subtitle JSON files into df_lines\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "SE_RE = re.compile(r\"[Ss](\\d{1,2})[Ee](\\d{1,2})\")\n",
        "\n",
        "def parse_episode_key(ep_key: str):\n",
        "    \"\"\"Extract S/E numbers from filenames like S01E04.\"\"\"\n",
        "    m = SE_RE.search(ep_key)\n",
        "    season = int(m.group(1)) if m else None\n",
        "    episode = int(m.group(2)) if m else None\n",
        "    return season, episode\n",
        "\n",
        "def load_kaggle_season_json(p: Path, seconds_per_line: float = 2.5) -> list[dict]:\n",
        "    \"\"\"Convert seasonN.json into line-level timestamped text rows.\"\"\"\n",
        "    with open(p, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)  # {episode_filename: { \"1\": line1, \"2\": line2, ... }}\n",
        "\n",
        "    rows = []\n",
        "    for ep_key, lines_obj in data.items():\n",
        "        season, episode = parse_episode_key(ep_key)\n",
        "\n",
        "        # Sort numerically\n",
        "        items = sorted(lines_obj.items(), key=lambda kv: int(kv[0]) if kv[0].isdigit() else kv[0])\n",
        "\n",
        "        for idx, (_, text) in enumerate(items):\n",
        "            if not text or not str(text).strip():\n",
        "                continue\n",
        "\n",
        "            t_start = idx * seconds_per_line\n",
        "            t_end = t_start + seconds_per_line\n",
        "\n",
        "            rows.append({\n",
        "                \"season\": season,\n",
        "                \"episode\": episode,\n",
        "                \"t_start\": float(t_start),\n",
        "                \"t_end\": float(t_end),\n",
        "                \"text\": str(text).strip()\n",
        "            })\n",
        "\n",
        "    return rows\n",
        "\n",
        "# Ingest all season JSON files\n",
        "all_lines = []\n",
        "for p in sorted(RAW_JSON_DIR.glob(\"season*.json\")):\n",
        "    print(f\"üì• Loading {p.name} ...\")\n",
        "    all_lines.extend(load_kaggle_season_json(p))\n",
        "\n",
        "df_lines = pd.DataFrame(all_lines).dropna(subset=[\"text\"])\n",
        "df_lines = df_lines.sort_values([\"season\",\"episode\",\"t_start\"]).reset_index(drop=True)\n",
        "print(f\"üé¨ Loaded {len(df_lines)} subtitle lines\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. Build subtitle chunks from df_lines\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def build_subtitle_chunks(df_lines, window=5, stride=3):\n",
        "    rows = []\n",
        "    for (season, episode), group in df_lines.groupby([\"season\", \"episode\"]):\n",
        "        texts  = group[\"text\"].tolist()\n",
        "        starts = group[\"t_start\"].tolist()\n",
        "        ends   = group[\"t_end\"].tolist()\n",
        "\n",
        "        for i in range(0, len(texts), stride):\n",
        "            chunk_text = \" \".join(texts[i:i+window]).strip()\n",
        "            if not chunk_text:\n",
        "                continue\n",
        "\n",
        "            chunk_start = starts[i]\n",
        "            chunk_end   = ends[min(i + window - 1, len(ends) - 1)]\n",
        "\n",
        "            rows.append({\n",
        "                \"season\": season,\n",
        "                \"episode\": episode,\n",
        "                \"t_start\": float(chunk_start),\n",
        "                \"t_end\": float(chunk_end),\n",
        "                \"text\": chunk_text,\n",
        "                \"chunk_kind\": \"subtitle\"\n",
        "            })\n",
        "\n",
        "    df_chunks = pd.DataFrame(rows).dropna(subset=[\"text\"])\n",
        "    print(f\"üéûÔ∏è Built {len(df_chunks)} subtitle chunks\")\n",
        "    return df_chunks\n",
        "\n",
        "df_chunks = build_subtitle_chunks(df_lines)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. Load Hugging Face character lore dataset\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "print(\"\\nüì• Loading Tuana/game-of-thrones (character lore)...\")\n",
        "ds = load_dataset(\"Tuana/game-of-thrones\", split=\"train\")\n",
        "df_lore = ds.to_pandas()\n",
        "\n",
        "def extract_name(meta):\n",
        "    if isinstance(meta, str):\n",
        "        try:\n",
        "            meta = ast.literal_eval(meta)\n",
        "        except:\n",
        "            return None\n",
        "    if isinstance(meta, dict):\n",
        "        return meta.get(\"name\", None)\n",
        "    return None\n",
        "\n",
        "df_lore[\"meta_name\"] = df_lore[\"meta\"].apply(extract_name)\n",
        "df_lore = df_lore.rename(columns={\"content\": \"text\"})\n",
        "df_lore = df_lore.dropna(subset=[\"text\"]).reset_index(drop=True)\n",
        "\n",
        "df_lore_chunks = pd.DataFrame({\n",
        "    \"season\": None,\n",
        "    \"episode\": None,\n",
        "    \"t_start\": 0.0,\n",
        "    \"t_end\": 0.0,\n",
        "    \"text\": df_lore[\"text\"].astype(str).str.strip(),\n",
        "    \"chunk_kind\": \"character_lore\"\n",
        "})\n",
        "\n",
        "print(f\"üìö Built {len(df_lore_chunks)} lore chunks\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. Merge subtitle + lore chunks\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "df_aug = pd.concat([df_chunks, df_lore_chunks], ignore_index=True)\n",
        "df_aug = df_aug.dropna(subset=[\"text\"]).reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nüîó Merged dataset now has {len(df_aug)} total chunks\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 5. Save snapshot for Phase 1-C (embeddings + FAISS)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "OUT = Path(\"data/got_augmented_lore.csv\")\n",
        "OUT.parent.mkdir(parents=True, exist_ok=True)\n",
        "df_aug.to_csv(OUT, index=False)\n",
        "print(f\"üíæ Saved enriched + chunked dataset ‚Üí {OUT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parsing and Normalizing Kaggle Subtitle JSON Files\n",
        "\n",
        "This notebook uses a helper script (`ingest_kaggle_json.py`) to convert the raw Kaggle subtitle JSON files into a clean, structured DataFrame. The raw dataset stores each episode as a large dictionary of numbered lines, so this step extracts season and episode identifiers, orders the lines correctly, and synthesizes simple timestamp fields to support downstream chunking and embedding steps.\n",
        "\n",
        "Each `season*.json` file is parsed into rows containing the season, episode, start time, end time, and the cleaned subtitle text. All seasons are merged into a single DataFrame, sorted chronologically, and filtered to keep Seasons 1 through 7. The resulting dataset becomes the standardized foundation for all further preprocessing, retrieval indexing, and narrative evaluation."
      ],
      "metadata": {
        "id": "XTCeE3jwHWyg"
      },
      "id": "XTCeE3jwHWyg"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "bd1675e5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bd1675e5",
        "outputId": "7422da43-dd79-4f94-eb30-05269c8a5b64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded lines: 44890\n",
            "   season  episode  t_start  t_end  \\\n",
            "0       1        1      0.0    2.5   \n",
            "1       1        1      2.5    5.0   \n",
            "2       1        1      5.0    7.5   \n",
            "3       1        1      7.5   10.0   \n",
            "4       1        1     10.0   12.5   \n",
            "\n",
            "                                                text  \n",
            "0                                         Easy, boy.  \n",
            "1               What do you expect? They're savages.  \n",
            "2            One lot steals a goat from another lot,  \n",
            "3  before you know it they're ripping each other ...  \n",
            "4    I've never seen wildlings do a thing like this.  \n"
          ]
        }
      ],
      "source": [
        "# ingest_kaggle_json.py\n",
        "from pathlib import Path\n",
        "import json, regex as re\n",
        "import pandas as pd\n",
        "\n",
        "RAW_JSON_DIR = Path(\"data\")  # put season1.json ... season7.json here\n",
        "\n",
        "# Extract S/E from \"Game Of Thrones S01E01 Winter Is Coming.srt\"\n",
        "SE_RE = re.compile(r\"[Ss](\\d{1,2})[Ee](\\d{1,2})\")\n",
        "\n",
        "def parse_episode_key(ep_key: str):\n",
        "    m = SE_RE.search(ep_key)\n",
        "    season = int(m.group(1)) if m else None\n",
        "    episode = int(m.group(2)) if m else None\n",
        "    return season, episode\n",
        "\n",
        "def load_kaggle_season_json(p: Path, seconds_per_line: float = 2.5) -> list[dict]:\n",
        "    with open(p, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)  # dict: episode_filename -> { \"1\": \"line\", \"2\": \"line\", ... }\n",
        "\n",
        "    rows = []\n",
        "    for ep_key, lines_obj in data.items():\n",
        "        season, episode = parse_episode_key(ep_key)\n",
        "        # lines_obj keys are strings of integers; sort numerically\n",
        "        line_items = sorted(lines_obj.items(), key=lambda kv: int(kv[0]) if kv[0].isdigit() else kv[0])\n",
        "        for idx, (_, text) in enumerate(line_items):\n",
        "            if not text or not str(text).strip():\n",
        "                continue\n",
        "            # synthesize simple timestamps so downstream chunkers work\n",
        "            t_start = idx * seconds_per_line\n",
        "            t_end   = t_start + seconds_per_line\n",
        "            rows.append({\n",
        "                \"season\": season,\n",
        "                \"episode\": episode,\n",
        "                \"t_start\": float(t_start),\n",
        "                \"t_end\": float(t_end),\n",
        "                \"text\": str(text).strip()\n",
        "            })\n",
        "    return rows\n",
        "\n",
        "# ---- Ingest all season*.json files ----\n",
        "all_rows = []\n",
        "for p in sorted(RAW_JSON_DIR.glob(\"season*.json\")):\n",
        "    all_rows.extend(load_kaggle_season_json(p, seconds_per_line=2.5))\n",
        "\n",
        "df_lines = pd.DataFrame(all_rows).dropna(subset=[\"text\"])\n",
        "# keep S1‚ÄìS7 if desired\n",
        "df_lines = df_lines[(df_lines[\"season\"] >= 1) & (df_lines[\"season\"] <= 7)]\n",
        "df_lines = df_lines.sort_values([\"season\", \"episode\", \"t_start\"]).reset_index(drop=True)\n",
        "\n",
        "print(\"Loaded lines:\", len(df_lines))\n",
        "print(df_lines.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FunTrivia Web Crawler: Collecting External Game of Thrones QA Pairs\n",
        "\n",
        "This section uses a robust web crawler to gather high-quality Game of Thrones trivia questions from FunTrivia.com.  \n",
        "It extracts structured fields such as the question number, question text, short answer, explanation, and the source URL.\n",
        "\n",
        "The crawler performs the following steps:\n",
        "\n",
        "1. **HTML Parsing of Question Blocks**  \n",
        "   Each page is scanned for structured `schema.org/Question` entries. The parser extracts the question text, the accepted answer, and any explanatory text, handling many variations in page formatting.\n",
        "\n",
        "2. **Iterative Pagination Across Trivia Pages**  \n",
        "   The crawler automatically identifies ‚Äúnext page‚Äù links using multiple fallback strategies. This ensures reliable traversal across FunTrivia‚Äôs multi-page trivia sets, even when pagination markup varies.\n",
        "\n",
        "3. **Duplicate Prevention and Safety Guards**  \n",
        "   Each (qnum, question) pair is tracked to avoid duplicates. The crawler enforces limits on maximum questions, maximum pages, and includes polite delays between requests.\n",
        "\n",
        "4. **Dataset Assembly and Export**  \n",
        "   All extracted records are compiled into a DataFrame and saved as  \n",
        "   `data/funtrivia_questions_all_200.csv`.  \n",
        "   These external QA pairs are later used for narrative evaluation, cross-checking retrieval, or augmenting the golden dataset.\n",
        "\n",
        "This crawler enables the notebook to incorporate additional grounded, human-written GOT questions that serve as a valuable benchmark for retrieval and answer-generation quality."
      ],
      "metadata": {
        "id": "qZVFvH02KDCg"
      },
      "id": "qZVFvH02KDCg"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "51c8a237",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "51c8a237",
        "outputId": "65263e88-8294-4e5e-e5e1-19e93fb78542"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped 165 QA pairs -> data/funtrivia_questions_all_200.csv\n",
            "Scraped 165 QA pairs -> data/funtrivia_questions_all_200.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  qnum                                           question        answer_short  \\\n",
              "0    1  Who are the two brothers of the Night's Watch ...     Brant and Derek   \n",
              "1    2  Daenerys is traded away as a bride to a \"savag...             Viserys   \n",
              "2    3  The Night King has so far been the main villai...            Viserion   \n",
              "3    4  At the conclusion of season 7, Cersei Lanniste...  The Golden Company   \n",
              "4    5                What is the name of Jon's direwolf?               Ghost   \n",
              "\n",
              "                                         explanation  \\\n",
              "0  Sam is beaten half to death trying to defend G...   \n",
              "1  Prince Viserys (Harry Lloyd) is the older and ...   \n",
              "2  The Night King kills Viserion with a fantastic...   \n",
              "3  The Golden Company is an an elite group of sel...   \n",
              "4  Ghost, unlike the other direwolves found by th...   \n",
              "\n",
              "                                          source_url  \n",
              "0  https://www.funtrivia.com/en/Television/Game-o...  \n",
              "1  https://www.funtrivia.com/en/Television/Game-o...  \n",
              "2  https://www.funtrivia.com/en/Television/Game-o...  \n",
              "3  https://www.funtrivia.com/en/Television/Game-o...  \n",
              "4  https://www.funtrivia.com/en/Television/Game-o...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d82386ac-84d3-431a-9ddd-6a8c12a63e75\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qnum</th>\n",
              "      <th>question</th>\n",
              "      <th>answer_short</th>\n",
              "      <th>explanation</th>\n",
              "      <th>source_url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Who are the two brothers of the Night's Watch ...</td>\n",
              "      <td>Brant and Derek</td>\n",
              "      <td>Sam is beaten half to death trying to defend G...</td>\n",
              "      <td>https://www.funtrivia.com/en/Television/Game-o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Daenerys is traded away as a bride to a \"savag...</td>\n",
              "      <td>Viserys</td>\n",
              "      <td>Prince Viserys (Harry Lloyd) is the older and ...</td>\n",
              "      <td>https://www.funtrivia.com/en/Television/Game-o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>The Night King has so far been the main villai...</td>\n",
              "      <td>Viserion</td>\n",
              "      <td>The Night King kills Viserion with a fantastic...</td>\n",
              "      <td>https://www.funtrivia.com/en/Television/Game-o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>At the conclusion of season 7, Cersei Lanniste...</td>\n",
              "      <td>The Golden Company</td>\n",
              "      <td>The Golden Company is an an elite group of sel...</td>\n",
              "      <td>https://www.funtrivia.com/en/Television/Game-o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>What is the name of Jon's direwolf?</td>\n",
              "      <td>Ghost</td>\n",
              "      <td>Ghost, unlike the other direwolves found by th...</td>\n",
              "      <td>https://www.funtrivia.com/en/Television/Game-o...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d82386ac-84d3-431a-9ddd-6a8c12a63e75')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d82386ac-84d3-431a-9ddd-6a8c12a63e75 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d82386ac-84d3-431a-9ddd-6a8c12a63e75');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-b803a650-60fe-41b1-bc09-8ee33389ba33\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b803a650-60fe-41b1-bc09-8ee33389ba33')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-b803a650-60fe-41b1-bc09-8ee33389ba33 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_all",
              "summary": "{\n  \"name\": \"df_all\",\n  \"rows\": 165,\n  \"fields\": [\n    {\n      \"column\": \"qnum\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 165,\n        \"samples\": [\n          \"136\",\n          \"116\",\n          \"132\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 165,\n        \"samples\": [\n          \"How many kings did Tywin Lannister serve as the Hand?\",\n          \"Which Houses have black as the main color of their sigil?\",\n          \"Oberyn Martell died championing Tyrion Lannister in a \\\"trial by combat.\\\" By what nickname was his killer known?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_short\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 151,\n        \"samples\": [\n          \"House of the Undying\",\n          \"Mirri Maz Duur the witch\",\n          \"Qarth, Astapor and Yunkai\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"explanation\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 165,\n        \"samples\": [\n          \"Aerys II Targaeryn was the first king Tywin served as a hand for. After Robert's death, Tywin was made Hand of Joffrey I Baratheon. After his death, he briefly served Tommen I Baratheon, before his own murder.\",\n          \"House Harlaw's sigil is a white scythe on a black background and the Stonetree is a tree on a black background as well. Other Houses have black as the main color of their crests, such as Greyjoy, Karstark, Lynderly, Manwoody or Rogers.\",\n          \"Gregor \\\"The Mountain That Rides\\\" Clegane crushed the head of the Dornish prince, but not before being infected with a poison-dipped spear.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source_url\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          \"https://www.funtrivia.com/en/Television/Game-of-Thrones-20275.html\",\n          \"https://www.funtrivia.com/en/Television/Game-of-Thrones-20275_2.html\",\n          \"https://www.funtrivia.com/en/Television/Game-of-Thrones-20275_6.html\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Jupyter cell: robust FunTrivia GOT crawler ‚Äî extracts qnum, question, short answer, explanation, source_url\n",
        "\n",
        "import requests, time, urllib.parse\n",
        "from bs4 import BeautifulSoup\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (compatible; funtrivia-scraper/1.0; +https://github.com/you)\",\n",
        "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "}\n",
        "BASE = \"https://www.funtrivia.com\"\n",
        "START = \"https://www.funtrivia.com/en/Television/Game-of-Thrones-20275.html\"\n",
        "\n",
        "def parse_questions_from_soup(soup, source_url):\n",
        "    out = []\n",
        "    for q_div in soup.find_all(attrs={\"itemtype\":\"http://schema.org/Question\"}):\n",
        "        # number\n",
        "        step = q_div.find(class_=\"step\")\n",
        "        qnum = step.get_text(strip=True) if step else None\n",
        "\n",
        "        # question text (itemprop=name)\n",
        "        qname = q_div.find(attrs={\"itemprop\":\"name\"})\n",
        "        question = qname.get_text(\" \", strip=True) if qname else None\n",
        "\n",
        "        # accepted answer block\n",
        "        ans_bq = q_div.find(\"blockquote\", class_=\"answer\")\n",
        "        short_ans = \"\"\n",
        "        explanation = \"\"\n",
        "        if ans_bq:\n",
        "            # find all itemprop=\"text\" elements inside blockquote (first often contains short answer, second explanation)\n",
        "            texts = ans_bq.find_all(attrs={\"itemprop\":\"text\"})\n",
        "            if texts:\n",
        "                # Prefer bold inside first text for concise answer\n",
        "                first = texts[0]\n",
        "                b = first.find(\"b\")\n",
        "                if b and b.get_text(strip=True):\n",
        "                    short_ans = b.get_text(strip=True)\n",
        "                else:\n",
        "                    # fallback: strip \"Answer:\" prefix if present\n",
        "                    t0 = first.get_text(\" \", strip=True)\n",
        "                    short_ans = t0.replace(\"Answer:\", \"\").strip()\n",
        "                if len(texts) > 1:\n",
        "                    explanation = texts[1].get_text(\" \", strip=True)\n",
        "                else:\n",
        "                    # sometimes explanation is directly after the bold inside same node\n",
        "                    # try to remove the bold text from first node and use remaining as explanation\n",
        "                    if b:\n",
        "                        # remove bold tag content to get any trailing explanation text\n",
        "                        for tag in first.find_all(\"b\"):\n",
        "                            tag.decompose()\n",
        "                        rem = first.get_text(\" \", strip=True)\n",
        "                        if rem:\n",
        "                            explanation = rem\n",
        "            else:\n",
        "                # last-resort: take blockquote text and try to split first sentence as short answer\n",
        "                full = ans_bq.get_text(\" \", strip=True)\n",
        "                if \":\" in full:\n",
        "                    # common \"Answer: X Explanation...\"\n",
        "                    parts = full.split(\":\", 1)\n",
        "                    short_ans = parts[1].split()[0]\n",
        "                    explanation = parts[1].strip()\n",
        "                else:\n",
        "                    short_ans = full\n",
        "        out.append({\n",
        "            \"qnum\": qnum,\n",
        "            \"question\": question,\n",
        "            \"answer_short\": short_ans,\n",
        "            \"explanation\": explanation,\n",
        "            \"source_url\": source_url\n",
        "        })\n",
        "    return out\n",
        "\n",
        "def crawl_funtrivia(start_url=START, max_q=200, max_pages=50, pause=0.9):\n",
        "    seen_q = set()\n",
        "    results = []\n",
        "    url = start_url\n",
        "    pages = 0\n",
        "    while url and pages < max_pages and len(results) < max_q:\n",
        "        pages += 1\n",
        "        try:\n",
        "            resp = requests.get(url, headers=HEADERS, timeout=20)\n",
        "            resp.raise_for_status()\n",
        "        except Exception as e:\n",
        "            print(f\"failed {url}: {e}\")\n",
        "            break\n",
        "        soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "        parsed = parse_questions_from_soup(soup, url)\n",
        "        for item in parsed:\n",
        "            key = (item[\"qnum\"], item[\"question\"])\n",
        "            if key in seen_q:\n",
        "                continue\n",
        "            seen_q.add(key)\n",
        "            results.append(item)\n",
        "            if len(results) >= max_q:\n",
        "                break\n",
        "\n",
        "        # find next page (link rel=\"next\" or pager anchors)\n",
        "        next_link = None\n",
        "        link_tag = soup.find(\"link\", rel=\"next\")\n",
        "        if link_tag and link_tag.get(\"href\"):\n",
        "            next_link = urllib.parse.urljoin(BASE, link_tag[\"href\"])\n",
        "        else:\n",
        "            # fallback: look for anchor with pattern _2.html etc.\n",
        "            pager = soup.select_one(\".pagelist, .pagelinks, .pages\")\n",
        "            if pager:\n",
        "                a_next = pager.find(\"a\", string=lambda s: s and s.strip().isdigit() and int(s.strip()) == pages+1)\n",
        "                if a_next and a_next.get(\"href\"):\n",
        "                    next_link = urllib.parse.urljoin(BASE, a_next[\"href\"])\n",
        "            # last fallback: look for any \"Next\" text\n",
        "            if not next_link:\n",
        "                a_next2 = soup.find(\"a\", string=lambda s: s and \"next\" in s.lower())\n",
        "                if a_next2 and a_next2.get(\"href\"):\n",
        "                    next_link = urllib.parse.urljoin(BASE, a_next2[\"href\"])\n",
        "\n",
        "        url = next_link\n",
        "        time.sleep(pause)\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    return df\n",
        "\n",
        "# run crawler (adjust max_q as needed)\n",
        "df_all = crawl_funtrivia(max_q=200, max_pages=40, pause=0.8)\n",
        "\n",
        "outdir = Path(\"data\")\n",
        "outdir.mkdir(parents=True, exist_ok=True)\n",
        "outpath = outdir / \"funtrivia_questions_all_200.csv\"\n",
        "df_all.to_csv(outpath, index=False)\n",
        "print(f\"Scraped {len(df_all)} QA pairs -> {outpath}\")\n",
        "df_all.head()\n",
        "#```# filepath: /Users/carlosrortiz/Documents/csc5830-ThroneRag/got_eda.ipynb\n",
        "# Jupyter cell: robust FunTrivia GOT crawler ‚Äî extracts qnum, question, short answer, explanation, source_url\n",
        "!pip install -q requests beautifulsoup4 lxml tqdm\n",
        "\n",
        "import requests, time, urllib.parse\n",
        "from bs4 import BeautifulSoup\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (compatible; funtrivia-scraper/1.0; +https://github.com/you)\",\n",
        "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "}\n",
        "BASE = \"https://www.funtrivia.com\"\n",
        "START = \"https://www.funtrivia.com/en/Television/Game-of-Thrones-20275.html\"\n",
        "\n",
        "def parse_questions_from_soup(soup, source_url):\n",
        "    out = []\n",
        "    for q_div in soup.find_all(attrs={\"itemtype\":\"http://schema.org/Question\"}):\n",
        "        # number\n",
        "        step = q_div.find(class_=\"step\")\n",
        "        qnum = step.get_text(strip=True) if step else None\n",
        "\n",
        "        # question text (itemprop=name)\n",
        "        qname = q_div.find(attrs={\"itemprop\":\"name\"})\n",
        "        question = qname.get_text(\" \", strip=True) if qname else None\n",
        "\n",
        "        # accepted answer block\n",
        "        ans_bq = q_div.find(\"blockquote\", class_=\"answer\")\n",
        "        short_ans = \"\"\n",
        "        explanation = \"\"\n",
        "        if ans_bq:\n",
        "            # find all itemprop=\"text\" elements inside blockquote (first often contains short answer, second explanation)\n",
        "            texts = ans_bq.find_all(attrs={\"itemprop\":\"text\"})\n",
        "            if texts:\n",
        "                # Prefer bold inside first text for concise answer\n",
        "                first = texts[0]\n",
        "                b = first.find(\"b\")\n",
        "                if b and b.get_text(strip=True):\n",
        "                    short_ans = b.get_text(strip=True)\n",
        "                else:\n",
        "                    # fallback: strip \"Answer:\" prefix if present\n",
        "                    t0 = first.get_text(\" \", strip=True)\n",
        "                    short_ans = t0.replace(\"Answer:\", \"\").strip()\n",
        "                if len(texts) > 1:\n",
        "                    explanation = texts[1].get_text(\" \", strip=True)\n",
        "                else:\n",
        "                    # sometimes explanation is directly after the bold inside same node\n",
        "                    # try to remove the bold text from first node and use remaining as explanation\n",
        "                    if b:\n",
        "                        # remove bold tag content to get any trailing explanation text\n",
        "                        for tag in first.find_all(\"b\"):\n",
        "                            tag.decompose()\n",
        "                        rem = first.get_text(\" \", strip=True)\n",
        "                        if rem:\n",
        "                            explanation = rem\n",
        "            else:\n",
        "                # last-resort: take blockquote text and try to split first sentence as short answer\n",
        "                full = ans_bq.get_text(\" \", strip=True)\n",
        "                if \":\" in full:\n",
        "                    # common \"Answer: X Explanation...\"\n",
        "                    parts = full.split(\":\", 1)\n",
        "                    short_ans = parts[1].split()[0]\n",
        "                    explanation = parts[1].strip()\n",
        "                else:\n",
        "                    short_ans = full\n",
        "        out.append({\n",
        "            \"qnum\": qnum,\n",
        "            \"question\": question,\n",
        "            \"answer_short\": short_ans,\n",
        "            \"explanation\": explanation,\n",
        "            \"source_url\": source_url\n",
        "        })\n",
        "    return out\n",
        "\n",
        "def crawl_funtrivia(start_url=START, max_q=200, max_pages=50, pause=0.9):\n",
        "    seen_q = set()\n",
        "    results = []\n",
        "    url = start_url\n",
        "    pages = 0\n",
        "    while url and pages < max_pages and len(results) < max_q:\n",
        "        pages += 1\n",
        "        try:\n",
        "            resp = requests.get(url, headers=HEADERS, timeout=20)\n",
        "            resp.raise_for_status()\n",
        "        except Exception as e:\n",
        "            print(f\"failed {url}: {e}\")\n",
        "            break\n",
        "        soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "        parsed = parse_questions_from_soup(soup, url)\n",
        "        for item in parsed:\n",
        "            key = (item[\"qnum\"], item[\"question\"])\n",
        "            if key in seen_q:\n",
        "                continue\n",
        "            seen_q.add(key)\n",
        "            results.append(item)\n",
        "            if len(results) >= max_q:\n",
        "                break\n",
        "\n",
        "        # find next page (link rel=\"next\" or pager anchors)\n",
        "        next_link = None\n",
        "        link_tag = soup.find(\"link\", rel=\"next\")\n",
        "        if link_tag and link_tag.get(\"href\"):\n",
        "            next_link = urllib.parse.urljoin(BASE, link_tag[\"href\"])\n",
        "        else:\n",
        "            # fallback: look for anchor with pattern _2.html etc.\n",
        "            pager = soup.select_one(\".pagelist, .pagelinks, .pages\")\n",
        "            if pager:\n",
        "                a_next = pager.find(\"a\", string=lambda s: s and s.strip().isdigit() and int(s.strip()) == pages+1)\n",
        "                if a_next and a_next.get(\"href\"):\n",
        "                    next_link = urllib.parse.urljoin(BASE, a_next[\"href\"])\n",
        "            # last fallback: look for any \"Next\" text\n",
        "            if not next_link:\n",
        "                a_next2 = soup.find(\"a\", string=lambda s: s and \"next\" in s.lower())\n",
        "                if a_next2 and a_next2.get(\"href\"):\n",
        "                    next_link = urllib.parse.urljoin(BASE, a_next2[\"href\"])\n",
        "\n",
        "        url = next_link\n",
        "        time.sleep(pause)\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    return df\n",
        "\n",
        "# run crawler (adjust max_q as needed)\n",
        "df_all = crawl_funtrivia(max_q=200, max_pages=40, pause=0.8)\n",
        "\n",
        "outdir = Path(\"data\")\n",
        "outdir.mkdir(parents=True, exist_ok=True)\n",
        "outpath = outdir / \"funtrivia_questions_all_200.csv\"\n",
        "df_all.to_csv(outpath, index=False)\n",
        "print(f\"Scraped {len(df_all)} QA pairs -> {outpath}\")\n",
        "df_all.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One-Time Retrieval Setup: Embeddings, FAISS Indexing, BM25, and Hybrid Search\n",
        "\n",
        "This section initializes the full retrieval backend used throughout the notebook.  \n",
        "It runs only once per session, or reruns automatically if the underlying chunk dataframe changes.\n",
        "\n",
        "The setup performs four major tasks:\n",
        "\n",
        "1. **Embedding Generation (or Loading Artifacts)**  \n",
        "   The system uses a single embedding model (`text-embedding-3-large`) and generates vector embeddings for all text chunks.  \n",
        "   If previously saved vector files or FAISS indexes are found in the `data/` directory, these are loaded instead of recomputed.\n",
        "\n",
        "2. **FAISS Index Construction**  \n",
        "   The vectors are normalized and stored in a FAISS inner-product index for efficient semantic search.  \n",
        "   If an index already exists, it is loaded and validated. If loading fails, the index is rebuilt automatically.\n",
        "\n",
        "3. **BM25 Token-Based Retrieval**  \n",
        "   Each chunk is tokenized and indexed using BM25 (RankBM25). This provides a complementary sparse retrieval pathway that helps catch lexical matches missed by embeddings.\n",
        "\n",
        "4. **Hybrid Search Initialization**  \n",
        "   A single `hybrid_search()` function combines FAISS similarity scores and BM25 scores to produce a ranked list of candidate chunks.  \n",
        "   It includes safety checks for index length mismatches, truncated vectors, missing embeddings, and out-of-range FAISS hits.\n",
        "\n",
        "Together, these components form the notebook‚Äôs unified retrieval layer, enabling fast, stable, and repeatable semantic + lexical search over the Game of Thrones subtitle corpus."
      ],
      "metadata": {
        "id": "7ge5a_aFH5WU"
      },
      "id": "7ge5a_aFH5WU"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# Phase 1-C ‚Äî Embeddings, FAISS, BM25\n",
        "# ==========================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Prerequisites:\n",
        "# Requires df_aug from Phase 1-B\n",
        "if \"df_aug\" not in globals():\n",
        "    raise RuntimeError(\"df_aug not found. Run Phase 1-B before Phase 1-C.\")\n",
        "\n",
        "BASE = Path(\"data\")\n",
        "BASE.mkdir(exist_ok=True)\n",
        "\n",
        "# Artifact paths\n",
        "VEC_PATH   = BASE / \"got_aug_vecs.npy\"\n",
        "INDEX_PATH = BASE / \"got_aug_faiss.bin\"\n",
        "TOK_PATH   = BASE / \"got_aug_corpus_tokens.pkl\"\n",
        "\n",
        "# -----------------------------------------\n",
        "# OpenAI setup\n",
        "# -----------------------------------------\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "if not OPENAI_KEY:\n",
        "    raise RuntimeError(\"OPENAI_API_KEY missing. Load .env or set manually.\")\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_KEY)\n",
        "EMBED_MODEL = \"text-embedding-3-large\"\n",
        "\n",
        "# -----------------------------------------\n",
        "# FAISS\n",
        "# -----------------------------------------\n",
        "import faiss\n",
        "\n",
        "# -----------------------------------------\n",
        "# spaCy for tokenization (BM25)\n",
        "# -----------------------------------------\n",
        "import spacy\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except:\n",
        "    raise RuntimeError(\"spaCy model missing. Run: python -m spacy download en_core_web_sm\")\n",
        "\n",
        "# -----------------------------------------\n",
        "# BM25\n",
        "# -----------------------------------------\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# -----------------------------------------\n",
        "# Helper embedder (safe batching)\n",
        "# -----------------------------------------\n",
        "def embed_texts(texts, batch_size=16, max_chars=8000):\n",
        "    vecs = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\"):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        batch_trunc = [\n",
        "            t if len(t) <= max_chars else t[:max_chars] + \" [TRUNCATED]\"\n",
        "            for t in batch\n",
        "        ]\n",
        "        try:\n",
        "            resp = client.embeddings.create(model=EMBED_MODEL, input=batch_trunc)\n",
        "            vecs.extend([e.embedding for e in resp.data])\n",
        "        except Exception as e:\n",
        "            print(f\"Batch {i} failed: {e}. Retrying per item...\")\n",
        "            for t in batch_trunc:\n",
        "                for attempt in range(3):\n",
        "                    try:\n",
        "                        r = client.embeddings.create(model=EMBED_MODEL, input=[t])\n",
        "                        vecs.extend([e.embedding for e in r.data])\n",
        "                        break\n",
        "                    except Exception:\n",
        "                        time.sleep(1 + attempt)\n",
        "                else:\n",
        "                    raise RuntimeError(\"Failed embedding despite retries.\")\n",
        "    return np.asarray(vecs, dtype=\"float32\")\n",
        "\n",
        "\n",
        "# -----------------------------------------\n",
        "# Step 1 ‚Äî Build/load embedding vectors\n",
        "# -----------------------------------------\n",
        "if VEC_PATH.exists():\n",
        "    print(f\"üì• Loading vectors: {VEC_PATH}\")\n",
        "    vecs = np.load(VEC_PATH)\n",
        "else:\n",
        "    print(\"üîÆ Computing embeddings for df_aug ...\")\n",
        "    texts = df_aug[\"text\"].fillna(\"\").tolist()\n",
        "    vecs = embed_texts(texts)\n",
        "    np.save(VEC_PATH, vecs)\n",
        "    print(f\"üíæ Saved embeddings ‚Üí {VEC_PATH}\")\n",
        "\n",
        "# -----------------------------------------\n",
        "# Step 2 ‚Äî Build/load FAISS index\n",
        "# -----------------------------------------\n",
        "if INDEX_PATH.exists():\n",
        "    print(f\"üì• Loading FAISS index: {INDEX_PATH}\")\n",
        "    try:\n",
        "        index = faiss.read_index(str(INDEX_PATH))\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Failed to load FAISS index ({e}). Rebuilding...\")\n",
        "        faiss.normalize_L2(vecs)\n",
        "        index = faiss.IndexFlatIP(vecs.shape[1])\n",
        "        index.add(vecs)\n",
        "        faiss.write_index(index, str(INDEX_PATH))\n",
        "else:\n",
        "    print(\"‚öôÔ∏è Building FAISS index...\")\n",
        "    faiss.normalize_L2(vecs)\n",
        "    index = faiss.IndexFlatIP(vecs.shape[1])\n",
        "    index.add(vecs)\n",
        "    faiss.write_index(index, str(INDEX_PATH))\n",
        "    print(f\"üíæ Saved FAISS index ‚Üí {INDEX_PATH}\")\n",
        "\n",
        "print(f\"FAISS ready ‚Üí {index.ntotal} vectors\")\n",
        "\n",
        "# -----------------------------------------\n",
        "# Step 3 ‚Äî Build/load BM25 token cache\n",
        "# -----------------------------------------\n",
        "if TOK_PATH.exists():\n",
        "    print(f\"üì• Loading BM25 tokens: {TOK_PATH}\")\n",
        "    with open(TOK_PATH, \"rb\") as f:\n",
        "        corpus_tokens = pickle.load(f)\n",
        "else:\n",
        "    print(\"üß† Tokenizing for BM25 ...\")\n",
        "    corpus_tokens = [\n",
        "        [tok.text.lower() for tok in nlp(txt)]\n",
        "        for txt in tqdm(df_aug[\"text\"].fillna(\"\"), desc=\"BM25 Tokenizing\")\n",
        "    ]\n",
        "    with open(TOK_PATH, \"wb\") as f:\n",
        "        pickle.dump(corpus_tokens, f)\n",
        "    print(f\"üíæ Saved BM25 tokens ‚Üí {TOK_PATH}\")\n",
        "\n",
        "bm25 = BM25Okapi(corpus_tokens)\n",
        "print(\"BM25 ready.\")\n",
        "\n",
        "# -----------------------------------------\n",
        "# Final outputs of Phase 1-C\n",
        "# -----------------------------------------\n",
        "print(\"\\nüéâ Phase 1-C complete.\")\n",
        "print(\"Artifacts available:\")\n",
        "print(\" - df_aug\")\n",
        "print(f\" - {VEC_PATH.name}\")\n",
        "print(f\" - {INDEX_PATH.name}\")\n",
        "print(f\" - {TOK_PATH.name}\")\n",
        "print(\"\\nYou may now run Phase 1-D (fast loader) or your LangGraph nodes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ZNpW08j0UrTQ",
        "outputId": "8ee22166-e264-4a54-ce86-38ee17b24883"
      },
      "id": "ZNpW08j0UrTQ",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Loading vectors: data/got_aug_vecs.npy\n",
            "üì• Loading FAISS index: data/got_aug_faiss.bin\n",
            "FAISS ready ‚Üí 17347 vectors\n",
            "üì• Loading BM25 tokens: data/got_aug_corpus_tokens.pkl\n",
            "BM25 ready.\n",
            "\n",
            "üéâ Phase 1-C complete.\n",
            "Artifacts available:\n",
            " - df_aug\n",
            " - got_aug_vecs.npy\n",
            " - got_aug_faiss.bin\n",
            " - got_aug_corpus_tokens.pkl\n",
            "\n",
            "You may now run Phase 1-D (fast loader) or your LangGraph nodes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 1-D ‚Äî Fast Loader (Skip Embedding, Skip Tokenization, Skip FAISS Builds)\n",
        "\n",
        "This cell is a **quick-start environment loader**.  \n",
        "It loads all the *precomputed* artifacts produced earlier in Phase 1-C so you can immediately\n",
        "start running the RAG pipeline, LangGraph-style agent, evaluations, or UI tests **without\n",
        "spending time recomputing embeddings or building FAISS/BM25**.\n",
        "\n",
        "You only need to run Phase 1-C **once** (or whenever the dataset changes).  \n",
        "After that, this loader cell acts as your **instant startup**.\n",
        "\n",
        "---\n",
        "\n",
        "## What This Cell Does\n",
        "\n",
        "### 1. Loads All Retrieval Artifacts from Disk\n",
        "The cell loads (whichever version exists):\n",
        "\n",
        "- `df_aug` (combined subtitles + lore chunks)\n",
        "- `got_aug_vecs.npy` (embeddings)\n",
        "- `got_aug_faiss.bin` (FAISS index)\n",
        "- `got_aug_corpus_tokens.pkl` (BM25 tokenized corpus)\n",
        "\n",
        "It chooses the best available file using path fallbacks, so you never have to manually change paths.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Reconstructs All Retrieval Objects in Memory\n",
        "Once artifacts are loaded from disk, it reconstructs:\n",
        "\n",
        "- The **FAISS index**\n",
        "- The **BM25Okapi** scorer\n",
        "- The **hybrid_search_aug** retrieval function  \n",
        "- The OpenAI client (`client`)\n",
        "- The spaCy NLP pipeline (`nlp`)\n",
        "\n",
        "This produces a full, ready-to-use retrieval stack **exactly like the state produced by running Phase 1-C**.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Guards Against Missing Artifacts\n",
        "If any critical file is missing (DF, vectors, FAISS, BM25),\n",
        "the loader throws **clear diagnostic errors** so you know which Phase 1-C step must be rerun.\n",
        "\n",
        "This avoids silent failures and prevents mismatched vector length issues.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Re-registers Required Global Variables\n",
        "The notebook cells that follow expect the following to exist:\n",
        "\n",
        "- `df_aug`\n",
        "- `index`\n",
        "- `bm25`\n",
        "- `client`\n",
        "- `nlp`\n",
        "- `EMBED_MODEL`\n",
        "\n",
        "This loader explicitly sets all these globals so the rest of the notebook works without modification.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Optional Cross-Encoder Reranker\n",
        "If available, it loads the sentence-transformers cross-encoder:"
      ],
      "metadata": {
        "id": "6cEVDQ0ceVER"
      },
      "id": "6cEVDQ0ceVER"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d38218fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "d38218fb",
        "outputId": "97f29392-933b-45b1-e381-94a7bde15855"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataframe from data/got_augmented_lore.csv (17347 rows)\n",
            "Loaded vectors from data/got_aug_vecs.npy ((17347, 3072))\n",
            "Loaded FAISS index from data/got_aug_faiss.bin (ntotal=17347)\n",
            "Loaded BM25 token cache from data/got_aug_corpus_tokens.pkl (17347 docs)\n",
            "BM25 index ready\n",
            "Loaded CrossEncoder reranker\n",
            "\n",
            "Setup complete. Globals available: client, nlp, df_aug, index, bm25, EMBED_MODEL\n"
          ]
        }
      ],
      "source": [
        "# language: python\n",
        "# Quick \"load everything\" helper to skip embedding / tokenizing steps.\n",
        "# Run this cell before the nodes cell.\n",
        "\n",
        "import os, pickle\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ...existing code...\n",
        "def hybrid_search_aug(query: str, topk: int = 10, alpha: float = 0.35, cand_mult: int = 20):\n",
        "    import numpy as np, pandas as pd\n",
        "    # require loader cell to have set: df_aug, index, bm25, client, nlp, EMBED_MODEL\n",
        "    if \"df_aug\" not in globals():\n",
        "        raise NameError(\"df_aug not found. Run the loader cell that loads precomputed artifacts first.\")\n",
        "    q_emb = client.embeddings.create(model=EMBED_MODEL, input=[query]).data[0].embedding\n",
        "    qv = np.asarray(q_emb, dtype=\"float32\")[None, :]\n",
        "    faiss.normalize_L2(qv)\n",
        "    D, I = index.search(qv, topk * cand_mult)\n",
        "    vec_scores, vec_idx = D[0].tolist(), I[0].tolist()\n",
        "\n",
        "    q_tokens = [t.text.lower() for t in nlp(query)]\n",
        "    bm_scores = bm25.get_scores(q_tokens)\n",
        "    bm_top = np.argsort(bm_scores)[::-1][:topk * cand_mult]\n",
        "\n",
        "    max_valid = len(df_aug)\n",
        "    valid_vec_pairs = [\n",
        "        (int(idx), float(score))\n",
        "        for idx, score in zip(vec_idx, vec_scores)\n",
        "        if isinstance(idx, (int, np.integer)) and 0 <= int(idx) < max_valid\n",
        "    ]\n",
        "    v_map = {i: s for i, s in valid_vec_pairs}\n",
        "    vec_idx_valid = [i for i, _ in valid_vec_pairs]\n",
        "    bm_top_valid = [int(i) for i in bm_top if 0 <= int(i) < max_valid]\n",
        "\n",
        "    cand = list(set(vec_idx_valid) | set(bm_top_valid))\n",
        "    if not cand:\n",
        "        return pd.DataFrame([])\n",
        "\n",
        "    bm_max = max(bm_scores) if len(bm_scores) else 1.0\n",
        "    scored = []\n",
        "    for i in cand:\n",
        "        v = float(v_map.get(int(i), 0.0))\n",
        "        b = float(bm_scores[int(i)]) / (bm_max + 1e-6)\n",
        "        scored.append((int(i), alpha * v + (1 - alpha) * b))\n",
        "    scored.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    rows = []\n",
        "    for i, sc in scored[:topk]:\n",
        "        r = df_aug.iloc[int(i)].to_dict()\n",
        "        r[\"score\"] = float(sc)\n",
        "        rows.append(r)\n",
        "    return pd.DataFrame(rows)\n",
        "# ...existing code...\n",
        "\n",
        "# OpenAI client (same API used in notebook)\n",
        "try:\n",
        "    from openai import OpenAI\n",
        "except Exception:\n",
        "    raise RuntimeError(\"openai package or OpenAI import not available. Install 'openai' and retry.\")\n",
        "OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "if not OPENAI_KEY:\n",
        "    raise RuntimeError(\"OPENAI_API_KEY not set in environment. Set it or run dotenv load step.\")\n",
        "client = OpenAI(api_key=OPENAI_KEY)\n",
        "\n",
        "# spaCy\n",
        "try:\n",
        "    import spacy\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"spaCy model not ready: {e}. Run `python -m spacy download en_core_web_sm` if needed.\")\n",
        "\n",
        "# BM25 and FAISS\n",
        "try:\n",
        "    from rank_bm25 import BM25Okapi\n",
        "except Exception:\n",
        "    raise RuntimeError(\"rank_bm25 not installed. pip install rank-bm25\")\n",
        "\n",
        "try:\n",
        "    import faiss\n",
        "except Exception:\n",
        "    raise RuntimeError(\"faiss-cpu not installed. pip install faiss-cpu\")\n",
        "\n",
        "# Candidate artifact paths (prefer augmented artifacts)\n",
        "BASE = Path(\"data\")\n",
        "DF_AUG_PATHS = [BASE / \"got_aug_chunks.csv\", BASE / \"got_augmented_lore.csv\", BASE / \"got_aug_chunks.csv\"]\n",
        "DF_CHUNK_PATHS = [BASE / \"got_chunks.csv\", BASE / \"got_chunks.csv\", BASE / \"got_chunks.csv\"]\n",
        "VEC_PATHS = [BASE / \"got_aug_vecs.npy\", BASE / \"got_vecs.npy\", BASE / \"got_chunks_vecs.npy\"]\n",
        "IDX_PATHS = [BASE / \"got_aug_faiss.bin\", BASE / \"got_faiss.bin\", BASE / \"got_chunks_faiss.bin\"]\n",
        "TOK_PATHS = [BASE / \"got_aug_corpus_tokens.pkl\", BASE / \"got_corpus_tokens.pkl\", BASE / \"got_corpus_tokens.pkl\"]\n",
        "\n",
        "# load dataframe (prefer augmented)\n",
        "df_aug = None\n",
        "for p in DF_AUG_PATHS + DF_CHUNK_PATHS:\n",
        "    if p.exists():\n",
        "        df_aug = pd.read_csv(p)\n",
        "        print(f\"Loaded dataframe from {p} ({len(df_aug)} rows)\")\n",
        "        break\n",
        "if df_aug is None:\n",
        "    raise FileNotFoundError(\"No chunk dataframe found. Expected one of: \" + \", \".join(str(p) for p in DF_AUG_PATHS + DF_CHUNK_PATHS))\n",
        "\n",
        "# load vectors & index\n",
        "vec_path = next((p for p in VEC_PATHS if p.exists()), None)\n",
        "idx_path = next((p for p in IDX_PATHS if p.exists()), None)\n",
        "\n",
        "if vec_path is None and idx_path is None:\n",
        "    raise FileNotFoundError(\"No precomputed vectors or FAISS index found in data/. Place got_aug_vecs.npy/got_aug_faiss.bin (or equivalents) in data/\")\n",
        "\n",
        "if vec_path is not None:\n",
        "    vecs = np.load(vec_path)\n",
        "    print(f\"Loaded vectors from {vec_path} ({vecs.shape})\")\n",
        "else:\n",
        "    vecs = None\n",
        "\n",
        "if idx_path is not None:\n",
        "    try:\n",
        "        index = faiss.read_index(str(idx_path))\n",
        "        print(f\"Loaded FAISS index from {idx_path} (ntotal={index.ntotal})\")\n",
        "    except Exception as e:\n",
        "        if vecs is None:\n",
        "            raise RuntimeError(f\"Failed to read FAISS index and no vectors to rebuild: {e}\")\n",
        "        faiss.normalize_L2(vecs)\n",
        "        index = faiss.IndexFlatIP(vecs.shape[1])\n",
        "        index.add(vecs)\n",
        "        print(\"Rebuilt FAISS index from vectors\")\n",
        "else:\n",
        "    # build index from vecs\n",
        "    if vecs is None:\n",
        "        raise FileNotFoundError(\"No FAISS index path and no vector file to build from.\")\n",
        "    faiss.normalize_L2(vecs)\n",
        "    index = faiss.IndexFlatIP(vecs.shape[1])\n",
        "    index.add(vecs)\n",
        "    print(\"Built FAISS index from vectors (no index file present)\")\n",
        "\n",
        "# load BM25 token cache\n",
        "tok_path = next((p for p in TOK_PATHS if p.exists()), None)\n",
        "corpus_tokens = None\n",
        "if tok_path:\n",
        "    with open(tok_path, \"rb\") as f:\n",
        "        corpus_tokens = pickle.load(f)\n",
        "    print(f\"Loaded BM25 token cache from {tok_path} ({len(corpus_tokens)} docs)\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"BM25 token cache not found (expected data/got_aug_corpus_tokens.pkl or similar). If you want to rebuild tokens run the tokenization cell once.\")\n",
        "\n",
        "# init BM25\n",
        "bm25 = BM25Okapi(corpus_tokens)\n",
        "print(\"BM25 index ready\")\n",
        "\n",
        "# set standard globals used by notebook cells\n",
        "EMBED_MODEL = globals().get(\"EMBED_MODEL\", \"text-embedding-3-large\")\n",
        "_GOT_RETRIEVAL_SETUP_DONE = True\n",
        "_bm25_row_count = len(df_aug)\n",
        "\n",
        "# optional reranker placeholder (keeps behavior consistent)\n",
        "RERANKER = None\n",
        "try:\n",
        "    from sentence_transformers import CrossEncoder\n",
        "    RERANKER = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "    print(\"Loaded CrossEncoder reranker\")\n",
        "except Exception:\n",
        "    print(\"CrossEncoder reranker not loaded (optional)\")\n",
        "\n",
        "print(\"\\nSetup complete. Globals available: client, nlp, df_aug, index, bm25, EMBED_MODEL\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 3 ‚Äî COSINE of Thrones Agentic RAG Scaffold\n",
        "\n",
        "This cell defines a **custom, hand-built agentic RAG pipeline** that behaves like a\n",
        "mini-LangGraph workflow, but without using **LangChain**, **LangGraph**, or any external\n",
        "orchestration libraries. The entire graph is constructed manually using plain Python,\n",
        "dataclasses, and simple function composition.\n",
        "\n",
        "## What This Cell Implements\n",
        "\n",
        "### 1. LangGraph-style State Object\n",
        "A shared `RAGState` dataclass carries information through the pipeline:\n",
        "- the user question  \n",
        "- detected question type  \n",
        "- named entities extracted from spaCy  \n",
        "- retrieved evidence chunks  \n",
        "- optional cross-encoder reranking  \n",
        "- formatted evidence text  \n",
        "- final grounded answer  \n",
        "- debug logs for each node  \n",
        "\n",
        "This mirrors LangGraph‚Äôs idea of a mutable ‚Äústate‚Äù flowing through nodes.\n",
        "\n",
        "### 2. Four Agentic Nodes (Your Own Implementation)\n",
        "The pipeline is built from four pure-Python nodes:\n",
        "\n",
        "1. **Query Parser**  \n",
        "   Extracts named entities and question type (who/what/when/etc.).  \n",
        "   Adds structured metadata to the state for downstream routing.\n",
        "\n",
        "2. **Retriever**  \n",
        "   Calls your **hybrid_search_aug** function which combines FAISS, BM25, and optional\n",
        "   entity boosting over the augmented dataset (subtitles + lore).  \n",
        "   Stores the resulting dataframe into `state.retrieved`.\n",
        "\n",
        "3. **Reranker (Optional)**  \n",
        "   If the cross-encoder model is available, it reranks chunks by semantic similarity,\n",
        "   providing a higher-quality evidence list. Otherwise retrieval order is preserved.\n",
        "\n",
        "4. **Synthesizer (LLM Agent)**  \n",
        "   Formats evidence and the question into a grounded, citation-first prompt.  \n",
        "   Calls OpenAI to produce a concise answer that only uses retrieved text.  \n",
        "   Writes the final answer into `state.answer`.\n",
        "\n",
        "Each node is a plain Python function that takes and returns `RAGState`, exactly like a\n",
        "LangGraph node, but without relying on an external graph engine.\n",
        "\n",
        "### 3. A Manual \"Graph Runner\"\n",
        "The `run_graph(question)` function executes the pipeline in a fixed order:"
      ],
      "metadata": {
        "id": "MGDx3VDYeBiC"
      },
      "id": "MGDx3VDYeBiC"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "440465a6",
      "metadata": {
        "id": "440465a6",
        "outputId": "fcae1c80-c021-4d19-a0a4-c98906fdbdf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reranker loaded: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
            "\n",
            "=======================\n",
            "üîç PROMPT SENT TO OPENAI\n",
            "=======================\n",
            "\n",
            "You are a Game of Thrones expert.\n",
            "Answer in one sentence, using ONLY the evidence lines below. If evidence is insufficient, say:\n",
            "\"I cannot find this in the provided evidence.\"\n",
            "Include season/episode if present.\n",
            "\n",
            "Question: Who is Jon Snow's mother in Game of Thrones?\n",
            "\n",
            "Evidence:\n",
            "[S?E?] ===Series reprisals=== * Jon Snow (Kit Harington), a member of the Night's Watch and bastard son of Ned Stark. * Cersei Lannister (Lena Headey), the Queen Regent of the Seven Kingdoms serving in King's Landing and mother to King Joffrey Baratheon. * Tyrion Lannister (Peter Dinklage), the Master of Coin serving in King's Landing and Queen Cersei's brother. * Ramsay Snow (Iwan Rheon), a sadistic and ruthless bastard son of Roose Bolton, the Warden of the North. * Daenerys Targaryen (Emilia Clarke), the Mother of Dragons and the potential future queen of the Seven Kingdoms, operating in Meereen. * Margaery Tyrell  (Natalie Dormer), a lady of Highgarden, betrothed to King Joffrey (who is not seen in the game, but is mentioned frequently).\n",
            "[S?E?] ===Jon Snow=== '''Jon Snow''' portrayed by Kit Harington. Kit Harington Jon Snow of House Stark and the Night's Watch is the secret son of Rhaegar Targaryen and Lyanna Stark, though raised as the bastard son of Lyanna's brother, Ned Stark. In the first season, Jon joins the Night's Watch. Jon is a talented fighter, but his sense of compassion and justice brings him into conflict with his harsh surroundings. Ned claims that Jon's mother was a wet nurse named Wylla. His dire wolf is called Ghost due to his albinism and quiet nature. Jon soon learns that the Watch is no longer a glorious order, but is now composed mostly of society's rejects, including criminals and exiles. Initially, he has only contempt for his low-born brothers of the Watch, but he puts aside his prejudices and befriends his fellow recruits, especially Samwell Tarly, after they unite against the cruel master-at-arms, Ser Alliser Thorne.\n",
            "[S7E3] Protector of the Seven Kingdoms, the Mother of Dragons, the Khaleesi of the Great Grass Sea, the Unburnt, the Breaker of Chains. This is Jon Snow.\n",
            "[S?E?] === Jon Snow === Jon Snow was raised as Ned Stark's illegitimate son and serves as the point of view character in 42 chapters throughout ''A Game of Thrones'', ''A Clash of Kings'', ''A Storm of Swords'', and ''A Dance with Dragons''. He shares the Stark family values of honour, and tries to stay morally correct and honest, even when forced to act otherwise.  He is theorized to be the son of Lyanna Stark, Ned Stark's sister, and Rhaegar Targaryen. In the HBO television adaptation, he is portrayed by Kit Harington.\n",
            "[S?E?] ===Parentage=== The identity of Jon's mother has created much speculation among readers of the series, and guessing her identity was the test Martin gave Benioff and Weiss when they approached him in March 2006 about adapting his novels into a TV series. In the novels, Martin hints that she could be a servant named Wylla, or the noblewoman Ashara Dayne. The popular fan theory‚Äîcalled \"R+L=J\", an abbreviation of \"Rhaegar + Lyanna = Jon\"‚Äîproposes that Jon is not the son of Ned at all, but is actually the son of Rhaegar Targaryen and Ned's younger sister Lyanna Stark. Though the character is presented as the illegitimate son of Ned Stark,  David Orr voiced the doubt of some readers when he wrote in ''The New York Times'' in 2011, \"Jon Snow is presented as the illegitimate son of the Stark patriarch, although it's uncertain whether Stark is indeed his father.\" Actor Sean Bean, who portrays Ned in the HBO television series, said when asked in a 2014 interview about returning to the series to appear in flashbacks, \"I've definitely got some unfinished business that needs to be resolved there. I'm obviously not Jon Snow's dad. And you need that to be revealed at some point, don't you?\" The uncertainty arises from anecdotal evidence in the texts interpreted by readers to connect the mysterious maternity of Ned's son with the vague backstory of his sister Lyanna. As recounted by Ned in ''A Game of Thrones'', at a tourney years before the events of the novel, Rhaegar had shown public favor to Lyanna in the presence of his own wife, the Dornish princess Elia Martell. When Rhaegar and Lyanna disappeared a year later, her father Rickard and eldest brother Brandon confronted Rhaegar's father, the Mad King Aerys Targaryen, demanding that his son return the abducted Lyanna. Aerys had Rickard and Brandon brutally executed for their insolence, inciting Ned and his friend Robert Baratheon, Lord of  Storm's End and Lyanna's betrothed, to rebel against Aerys. In what later became known as Robert's Rebellion, Aerys was overthrown and Rhaegar was killed by Robert in single combat. After a bloody battle against three of Aerys' Kingsguard protecting the Tower of Joy in Dorne, Ned found Lyanna inside, in a \"bed of blood.\" She died shortly after eliciting a promise from Ned. Once the war was won, he returned to Winterfell with his illegitimate son Jon. The R+L=J theory posits that rather than Rhaegar kidnapping Lyanna, they fell in love and ran away together. Living for a year in the Tower of Joy, they conceived a child‚ÄîJon. Rhaegar was killed in battle by Robert, and Lyanna died in childbirth. Ned promised Lyanna on her deathbed to claim the baby as his own to protect him from Robert, who sought to exterminate all Targaryens out of hatred and to secure his claim to the throne. HBO's ''Game of Thrones'' has included in its adaptation many of the \"hints\" identified by this theory. In the season 6 finale, \"The Winds of Winter\", Bran Stark has a vision of the past which shows Ned reuniting with a dying Lyanna in the Tower of Joy. Lyanna makes him promise to protect her son‚ÄîJon. An infographic subsequently posted on the HBO-controlled website MakingGameofThrones.com confirmed Rhaegar as Jon's father. Journalists later commented on the significance of two plot points in the season 7 episode \"Eastwatch\". One of Daenerys Targaryen's dragons, Drogon, approaches Jon calmly and allows the King in the North to pet him, seemingly recognizing him as a Targaryen. Later, Gilly learns from a book at the Citadel that a High Septon annulled Rhaegar's marriage, and married him to someone else in Dorne, suggesting the possibility that Jon is the legitimate son of Rhaegar and Lyanna. The season 7 finale episode \"The Dragon and the Wolf\" confirmed that Jon is indeed the legitimate son of Rhaegar and Lyanna, and that his birth name is actually Aegon Targaryen.\n",
            "\n",
            "\n",
            "=======================\n",
            "\n",
            "Q: Who is Jon Snow's mother in Game of Thrones?\n",
            "\n",
            "--- Evidence ---\n",
            "[S?E?] ===Series reprisals=== * Jon Snow (Kit Harington), a member of the Night's Watch and bastard son of Ned Stark. * Cersei Lannister (Lena Headey), the Queen Regent of the Seven Kingdoms serving in King's Landing and mother to King Joffrey Baratheon. * Tyrion Lannister (Peter Dinklage), the Master of Coin serving in King's Landing and Queen Cersei's brother. * Ramsay Snow (Iwan Rheon), a sadistic and ruthless bastard son of Roose Bolton, the Warden of the North. * Daenerys Targaryen (Emilia Clarke), the Mother of Dragons and the potential future queen of the Seven Kingdoms, operating in Meereen. * Margaery Tyrell  (Natalie Dormer), a lady of Highgarden, betrothed to King Joffrey (who is not seen in the game, but is mentioned frequently).\n",
            "[S?E?] ===Jon Snow=== '''Jon Snow''' portrayed by Kit Harington. Kit Harington Jon Snow of House Stark and the Night's Watch is the secret son of Rhaegar Targaryen and Lyanna Stark, though raised as the bastard son of Lyanna's brother, Ned Stark. In the first season, Jon joins the Night's Watch. Jon is a talented fighter, but his sense of compassion and justice brings him into conflict with his harsh surroundings. Ned claims that Jon's mother was a wet nurse named Wylla. His dire wolf is called Ghost due to his albinism and quiet nature. Jon soon learns that the Watch is no longer a glorious order, but is now composed mostly of society's rejects, including criminals and exiles. Initially, he has only contempt for his low-born brothers of the Watch, but he puts aside his prejudices and befriends his fellow recruits, especially Samwell Tarly, after they unite against the cruel master-at-arms, Ser Alliser Thorne.\n",
            "[S7E3] Protector of the Seven Kingdoms, the Mother of Dragons, the Khaleesi of the Great Grass Sea, the Unburnt, the Breaker of Chains. This is Jon Snow.\n",
            "[S?E?] === Jon Snow === Jon Snow was raised as Ned Stark's illegitimate son and serves as the point of view character in 42 chapters throughout ''A Game of Thrones'', ''A Clash of Kings'', ''A Storm of Swords'', and ''A Dance with Dragons''. He shares the Stark family values of honour, and tries to stay morally correct and honest, even when forced to act otherwise.  He is theorized to be the son of Lyanna Stark, Ned Stark's sister, and Rhaegar Targaryen. In the HBO television adaptation, he is portrayed by Kit Harington.\n",
            "[S?E?] ===Parentage=== The identity of Jon's mother has created much speculation among readers of the series, and guessing her identity was the test Martin gave Benioff and Weiss when they approached him in March 2006 about adapting his novels into a TV series. In the novels, Martin hints that she could be a servant named Wylla, or the noblewoman Ashara Dayne. The popular fan theory‚Äîcalled \"R+L=J\", an abbreviation of \"Rhaegar + Lyanna = Jon\"‚Äîproposes that Jon is not the son of Ned at all, but is actually the son of Rhaegar Targaryen and Ned's younger sister Lyanna Stark. Though the character is presented as the illegitimate son of Ned Stark,  David Orr voiced the doubt of some readers when he wrote in ''The New York Times'' in 2011, \"Jon Snow is presented as the illegitimate son of the Stark patriarch, although it's uncertain whether Stark is indeed his father.\" Actor Sean Bean, who portrays Ned in the HBO television series, said when asked in a 2014 interview about returning to the series to appear in flashbacks, \"I've definitely got some unfinished business that needs to be resolved there. I'm obviously not Jon Snow's dad. And you need that to be revealed at some point, don't you?\" The uncertainty arises from anecdotal evidence in the texts interpreted by readers to connect the mysterious maternity of Ned's son with the vague backstory of his sister Lyanna. As recounted by Ned in ''A Game of Thrones'', at a tourney years before the events of the novel, Rhaegar had shown public favor to Lyanna in the presence of his own wife, the Dornish princess Elia Martell. When Rhaegar and Lyanna disappeared a year later, her father Rickard and eldest brother Brandon confronted Rhaegar's father, the Mad King Aerys Targaryen, demanding that his son return the abducted Lyanna. Aerys had Rickard and Brandon brutally executed for their insolence, inciting Ned and his friend Robert Baratheon, Lord of  Storm's End and Lyanna's betrothed, to rebel against Aerys. In what later became known as Robert's Rebellion, Aerys was overthrown and Rhaegar was killed by Robert in single combat. After a bloody battle against three of Aerys' Kingsguard protecting the Tower of Joy in Dorne, Ned found Lyanna inside, in a \"bed of blood.\" She died shortly after eliciting a promise from Ned. Once the war was won, he returned to Winterfell with his illegitimate son Jon. The R+L=J theory posits that rather than Rhaegar kidnapping Lyanna, they fell in love and ran away together. Living for a year in the Tower of Joy, they conceived a child‚ÄîJon. Rhaegar was killed in battle by Robert, and Lyanna died in childbirth. Ned promised Lyanna on her deathbed to claim the baby as his own to protect him from Robert, who sought to exterminate all Targaryens out of hatred and to secure his claim to the throne. HBO's ''Game of Thrones'' has included in its adaptation many of the \"hints\" identified by this theory. In the season 6 finale, \"The Winds of Winter\", Bran Stark has a vision of the past which shows Ned reuniting with a dying Lyanna in the Tower of Joy. Lyanna makes him promise to protect her son‚ÄîJon. An infographic subsequently posted on the HBO-controlled website MakingGameofThrones.com confirmed Rhaegar as Jon's father. Journalists later commented on the significance of two plot points in the season 7 episode \"Eastwatch\". One of Daenerys Targaryen's dragons, Drogon, approaches Jon calmly and allows the King in the North to pet him, seemingly recognizing him as a Targaryen. Later, Gilly learns from a book at the Citadel that a High Septon annulled Rhaegar's marriage, and married him to someone else in Dorne, suggesting the possibility that Jon is the legitimate son of Rhaegar and Lyanna. The season 7 finale episode \"The Dragon and the Wolf\" confirmed that Jon is indeed the legitimate son of Rhaegar and Lyanna, and that his birth name is actually Aegon Targaryen.\n",
            "\n",
            "--- Answer ---\n",
            "Jon Snow's mother is Lyanna Stark.\n",
            "\n",
            "--- Logs ---\n",
            "parser => {'entities': [\"Jon Snow's\", 'Thrones'], 'question_type': 'who'}\n",
            "retriever => {'n_hits': 12}\n",
            "reranker => {'used': True, 'top_score': 5.737795352935791}\n",
            "synthesizer => {'prompt_length_chars': 6523, 'evidence_count': 12}\n"
          ]
        }
      ],
      "source": [
        "# ==========================\n",
        "# COSINE of Thrones ‚Äî Phase 3 Agentic RAG Scaffold (LangGraph-style)\n",
        "# ==========================\n",
        "# Prereqs expected in memory:\n",
        "# - nlp (spaCy English model)\n",
        "# - client = OpenAI(...)\n",
        "# - hybrid_search_aug(query, topk=10, alpha=0.6, cand_mult=20, use_ner_boost=True)\n",
        "# - GEN_MODEL (e.g., \"gpt-4o-mini\")\n",
        "# - ANSWER_PROMPT (or we define a concise one below)\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Dict, Any, Optional\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "GEN_MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "# ---- Optional cross-encoder reranker ----\n",
        "RERANKER = None\n",
        "try:\n",
        "    from sentence_transformers import CrossEncoder\n",
        "    RERANKER = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "    print(\"Reranker loaded: cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "except Exception as _e:\n",
        "    print(\"Reranker not available; continuing without cross-encoder.\")\n",
        "\n",
        "# ---- Minimal prompt (concise, citation-first) ----\n",
        "ANSWER_PROMPT = \"\"\"You are a Game of Thrones expert.\n",
        "Answer in one sentence, using ONLY the evidence lines below. If evidence is insufficient, say:\n",
        "\"I cannot find this in the provided evidence.\"\n",
        "Include season/episode if present.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Evidence:\n",
        "{evidence}\n",
        "\"\"\"\n",
        "\n",
        "# ---- Shared helpers ----\n",
        "def format_evidence_rows(df: pd.DataFrame, k: int = 5) -> str:\n",
        "    if df is None or len(df) == 0:\n",
        "        return \"(no evidence)\"\n",
        "    lines = []\n",
        "    for _, r in df.head(k).iterrows():\n",
        "        s, e = r.get(\"season\"), r.get(\"episode\")\n",
        "        tag = f\"S{int(s)}E{int(e)}\" if pd.notna(s) and pd.notna(e) else \"S?E?\"\n",
        "        txt = str(r.get(\"text\",\"\")).replace(\"\\n\",\" \").strip()\n",
        "        spk = r.get(\"speaker\")\n",
        "        prefix = f\"[{tag}] {spk}: \" if isinstance(spk, str) and spk.strip() else f\"[{tag}] \"\n",
        "        lines.append(prefix + txt)\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def guess_question_type(q: str) -> str:\n",
        "    ql = q.lower()\n",
        "    for t in [\"who\",\"what\",\"when\",\"where\",\"why\",\"how\",\"which\"]:\n",
        "        if ql.startswith(t) or f\" {t} \" in ql:\n",
        "            return t\n",
        "    return \"open\"\n",
        "\n",
        "def extract_entities(q: str) -> List[str]:\n",
        "    doc = nlp(q)\n",
        "    return [ent.text for ent in doc.ents]\n",
        "\n",
        "# ---- Graph State ----\n",
        "@dataclass\n",
        "class RAGState:\n",
        "    question: str\n",
        "    question_type: str = \"open\"\n",
        "    entities: List[str] = field(default_factory=list)\n",
        "    retrieved: Optional[pd.DataFrame] = None\n",
        "    reranked: Optional[pd.DataFrame] = None\n",
        "    evidence_text: str = \"\"\n",
        "    answer: str = \"\"\n",
        "    logs: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "# ---- Nodes ----\n",
        "def node_query_parser(state: RAGState) -> RAGState:\n",
        "    ents = extract_entities(state.question)\n",
        "    qtype = guess_question_type(state.question)\n",
        "    state.entities = ents\n",
        "    state.question_type = qtype\n",
        "    state.logs[\"parser\"] = {\"entities\": ents, \"question_type\": qtype}\n",
        "    return state\n",
        "\n",
        "def node_retriever(state: RAGState, topk: int = 12, alpha: float = 0.6) -> RAGState:\n",
        "    # Uses your augmented hybrid search\n",
        "    hits = hybrid_search_aug(state.question, topk=topk, alpha=alpha)\n",
        "    state.retrieved = hits\n",
        "    state.logs[\"retriever\"] = {\"n_hits\": 0 if hits is None else int(len(hits))}\n",
        "    return state\n",
        "\n",
        "def node_reranker(state: RAGState) -> RAGState:\n",
        "    if state.retrieved is None or len(state.retrieved) == 0 or RERANKER is None:\n",
        "        # fall back to retrieved as-is\n",
        "        state.reranked = state.retrieved\n",
        "        state.logs[\"reranker\"] = {\"used\": False, \"reason\": \"no-hits-or-no-model\"}\n",
        "        return state\n",
        "\n",
        "    pairs = [[state.question, t] for t in state.retrieved[\"text\"].tolist()]\n",
        "    scores = RERANKER.predict(pairs)\n",
        "    df = state.retrieved.copy().reset_index(drop=True)\n",
        "    df[\"rerank_score\"] = scores\n",
        "    df = df.sort_values(\"rerank_score\", ascending=False).reset_index(drop=True)\n",
        "    state.reranked = df\n",
        "    state.logs[\"reranker\"] = {\"used\": True, \"top_score\": float(df.iloc[0][\"rerank_score\"])}\n",
        "    return state\n",
        "\n",
        "def node_synthesizer(state: RAGState, k_evidence: int = 5, show_prompt: bool = True) -> RAGState:\n",
        "    \"\"\"\n",
        "    Generate grounded answer from retrieved evidence.\n",
        "    Optionally prints the full prompt (question + evidence) before sending to OpenAI.\n",
        "    \"\"\"\n",
        "    hits = state.reranked if state.reranked is not None else state.retrieved\n",
        "    ev_text = format_evidence_rows(hits, k=k_evidence)\n",
        "    state.evidence_text = ev_text\n",
        "\n",
        "    # ---- Construct the full prompt ----\n",
        "    prompt = ANSWER_PROMPT.format(question=state.question, evidence=ev_text)\n",
        "\n",
        "    # ---- Print for debugging ----\n",
        "    if show_prompt:\n",
        "        print(\"\\n=======================\")\n",
        "        print(\"üîç PROMPT SENT TO OPENAI\")\n",
        "        print(\"=======================\\n\")\n",
        "        print(prompt)\n",
        "        print(\"\\n=======================\\n\")\n",
        "\n",
        "    # ---- Call the model ----\n",
        "    out = client.chat.completions.create(\n",
        "        model=GEN_MODEL,\n",
        "        temperature=0.1,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "\n",
        "    state.answer = out.choices[0].message.content.strip()\n",
        "    state.logs[\"synthesizer\"] = {\n",
        "        \"prompt_length_chars\": len(prompt),\n",
        "        \"evidence_count\": len(hits) if hits is not None else 0\n",
        "    }\n",
        "    return state\n",
        "\n",
        "# ---- Graph runner ----\n",
        "def run_graph(question: str) -> RAGState:\n",
        "    state = RAGState(question=question)\n",
        "    # Ordered pipeline\n",
        "    state = node_query_parser(state)\n",
        "    state = node_retriever(state, topk=12, alpha=0.6)\n",
        "    state = node_reranker(state)\n",
        "    state = node_synthesizer(state, k_evidence=5)\n",
        "    return state\n",
        "\n",
        "# ==========================\n",
        "# Smoke test\n",
        "# ==========================\n",
        "sample_q = \"Who is Jon Snow's mother in Game of Thrones?\"\n",
        "st = run_graph(sample_q)\n",
        "\n",
        "print(\"Q:\", st.question)\n",
        "print(\"\\n--- Evidence ---\")\n",
        "print(st.evidence_text)\n",
        "print(\"\\n--- Answer ---\")\n",
        "print(st.answer)\n",
        "print(\"\\n--- Logs ---\")\n",
        "for k,v in st.logs.items():\n",
        "    print(k, \"=>\", v)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}